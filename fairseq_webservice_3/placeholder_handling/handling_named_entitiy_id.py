import re
import unicodedata
from typing import Tuple, Dict
import validators



latin_consonants = (
    "bcÃ§Ä‡ÄdÄ‘fgÄŸhjklÅ‚má¸¿nÃ±Å„Åˆpá¹•qrÅ•Å™sÅ›Å¡ÅÅŸÃŸtÅ£vwáºƒxzÅºÅ¾Å¼"
    "BCÃ‡Ä†ÄŒDÃFGÄHJKLÅMá¸¾NÅƒÅ‡Pá¹”QRÅ”Å˜SÅšÅ ÅœÅTÅ¢VWáº‚XZÅ»Å¹Å½"
)

latin_vowels = (
    "aÃ¡Ã¤Ã Ã¢Ã£Ã¥ÄÄƒÄ…Ã¦eÃ¨ÃªÃ«Ã©Ä›Ä™Ä“Ä—iÃ¬Ã­Ã®Ã¯Ä«ÇÄ±Ã¯oÃ¶Ã²Ã³Ã´ÃµÅÅÅ‘Ã¸Å“"
    "uÃ¼Ã¹ÃºÃ»Å«Å¯yÃ½Ã¿AÃ€Ã‚ÃƒÃ…Ã„ÃÄ€Ä‚Ä„Ã†Ã‰ÄšEÃŠÃ‹Ä˜Ä’Ä–IÃŒÃÃÃÄªÇÃÄ°"
    "OÃ–Ã“Ã’Ã”Ã•ÅŒÅÅÃ˜Å’UÃœÃ™ÃšÃ›ÅªÅ®ÃÅ¸Y"
)

# Regex fragments
regex_spaces = r"\s\u00A0"   # whitespace plus NBSP
regex_brackets = r"(){}\[\]"
regex_satzzeichen = r"[.;,?!:-]"

regex_interpunktion_nicht_satzzeichen = (
    r"\-/()<>=Â´`'\"\+\*\~:_\^"
    + r"Â«Â»â€˜â€™â€šâ€›â€œâ€â€â€Ÿâ€¹â€º"
)

regex_latin_plus_interpunktion = (
    latin_consonants
    + latin_vowels
    + regex_satzzeichen
    + regex_interpunktion_nicht_satzzeichen
    + regex_spaces
    + r"0-9"
    + r"@\$â‚¬"
)

# Pattern for marking non-latin
mark_nonlatin_pattern_str = (
    "("
    "[" + regex_latin_plus_interpunktion + "]*"
    ")"
    "("
    "[^" + regex_latin_plus_interpunktion + "]*"
    ")"
    "(.*)"
)
mark_nonlatin_pattern = re.compile(mark_nonlatin_pattern_str, re.DOTALL)

# Digit patterns
digit_chars = "0-9"
digit_interpunktion_chars = r"0-9\.;\-:,"

mark_ext_digit_pattern_str = (
    "([^" + digit_chars + "]*)"
    "("
    "[" + digit_chars + "]{0,1}"
    "[" + digit_interpunktion_chars + "]*"
    ")"
    "(.*)"
)
mark_ext_digit_pattern = re.compile(mark_ext_digit_pattern_str, re.DOTALL)
ESC_L = "â• "  # pseudo-escaped left marker
ESC_R = "â•£"  # pseudo-escaped right marker






def set_markers(text: str) -> Tuple[str, Dict[str, str]]:
    """
    Ersetzt URLs, Domains, E-Mails, Zahlenfolgen und nicht-lateinische Sequenzen
    durch interne NE-Marker der Form â”œ<type>:<id>â”¤.
    Gibt (marked_text, mapping) zurÃ¼ck, wobei mapping[id] = {'text': original, 'type': type}.
    """
    # ---------- 1) Pseudo-Escaping vorhandener NE-Marker ----------
    # Ersetze vorhandene "â”œ" / "â”¤" durch Ã¤hnliche Zeichen, die nicht als Marker interpretiert werden.
    text_ps = text.replace("â”œ", ESC_L).replace("â”¤", ESC_R)

    # ---------- Hilfsdaten ----------
    mapping: Dict[str, str] = {}
    used_ids = set()  # ids chosen so far (strings)
    # Sammle alle reinen Ziffernfolgen aus dem pseudo-escaped Text -> diese IDs vermeiden
    banned_digit_sequences = set(re.findall(r"\d+", text_ps))
    # generator fÃ¼r next available id (als string), Ã¼berspringt banned und used
    def next_id():
        i = 1
        while True:
            s = str(i)
            if s not in banned_digit_sequences and s not in used_ids:
                used_ids.add(s)
                return s
            i += 1

    # End-Satzzeichen, die am Tokenende entfernt/gesondert betrachtet werden sollen
    end_punct = set(".!?,;:â€”-()\"'")  # erweiterbar

    def is_non_latin_char(ch: str) -> bool:
        if ord(ch) <= 256:
            return False
        # treat punctuation etc. as latin
        if re.match(regex_satzzeichen, ch):
            return False
        try:
            nm = unicodedata.name(ch)
        except ValueError:
            # no name -> treat as non-latin (covers many symbols)
            return True
        # If Unicode name contains 'LATIN' -> treat as Latin
        return "LATIN" not in nm

    # ---------- Helper: split a non-space block into fragments of latin vs non-latin runs ----------
    def split_latin_nonlatin_runs(s: str):
        """
        Returns list of (substring, is_nonlatin) covering the original string.
        e.g. "testğŸ¥ªabcĞŸÑ€Ğ¸Ğ²ĞµÑ‚" -> [("test", False), ("ğŸ¥ª", True), ("abc", False), ("ĞŸÑ€Ğ¸Ğ²ĞµÑ‚", True)]
        """
        if not s:
            return []
        fragments = []
        cur_run = s[0]
        cur_flag = is_non_latin_char(s[0])
        for ch in s[1:]:
            flag = is_non_latin_char(ch)
            if flag == cur_flag:
                cur_run += ch
            else:
                fragments.append((cur_run, cur_flag))
                cur_run = ch
                cur_flag = flag
        fragments.append((cur_run, cur_flag))
        return fragments


    # ---------- Pipeline: split input preserving whitespace ----------
    parts = re.split(r"(\s+)", text_ps)  # Teile: tokens oder whitespace (whitespace in odd indices)

    # Wir bauen eine Liste result_parts, in der fÃ¼r jedes parts[i] entweder:
    # - a) {'text': original, 'ne': False}
    # - b) {'text': original, 'ne': True, 'type': <type>}

    intermediate = []

    for p in parts:
        if p.isspace() or p == "":
            intermediate.append({'text': p, 'ne': False})
            continue
        # p is a non-whitespace block; we will split it into latin/nonlatin runs
        runs = split_latin_nonlatin_runs(p)
        # Each run: (substring, is_nonlatin)
        for substring, is_nonlatin in runs:
            if is_nonlatin:
                intermediate.append({'text': substring, 'ne': True, 'type': 'nonlatin'})
            else:
                # latin/neutral run -> leave for further token-level checks (url/domain/email/number)
                intermediate.append({'text': substring, 'ne': False})



    #print("b: ", result_parts)
    # ---------- Schritt 3: FÃ¼r alle nicht-NE Fragmente -> Token-Splitting per Leerzeichen (schon segmentiert),
    # aber einzelne parts enthalten bereits keine Whitespace; wir verarbeiten diese non-NE-Parts tokenweise ----------
    final_parts = []
    #for item in result_parts:
    for item in intermediate:
        if item['ne'] or item['text'].isspace():
            final_parts.append(item)
            continue
        frag = item['text']  # dieser Block enthÃ¤lt keine non-latin chars und keine spaces
        # Entferne ggf. Satzzeichen am Ende des Tokens (schritt 3b)
        # Wir entfernen *fortlaufend* am Ende, aber sammeln sie, damit sie zum NE-Text gehÃ¶ren (ggf.)
        trailing = ''
        while frag and frag[-1] in end_punct:
            trailing = frag[-1] + trailing
            frag = frag[:-1]

        token_body = frag
        checked_as_ne = False

        # PrÃ¼fe Token auf URL / Email / Domain
        if token_body:
            if validators.email(token_body):
                # markiere als E-Mail (inkl. trailing Satzzeichen, wie in Schritt 3)
                final_parts.append({'text': token_body + trailing, 'ne': True, 'type': 'email'})
                checked_as_ne = True
            elif validators.url(token_body):
                final_parts.append({'text': token_body + trailing, 'ne': True, 'type': 'url'})
                checked_as_ne = True
            elif validators.domain(token_body):
                final_parts.append({'text': token_body + trailing, 'ne': True, 'type': 'domain'})
                checked_as_ne = True

        if not checked_as_ne:
            # Wenn nicht als NE erkannt, fÃ¼ge ursprÃ¼nglichen Token (inkl. trailing) als nicht-NE ein --
            # Schritt 4 wird danach auf alle noch-nicht-NE Fragmente angewendet.
            final_parts.append({'text': (token_body + trailing), 'ne': False})

    #print("c: ", final_parts)
    # ---------- Schritt 4: In noch-nicht-NE Fragmenten -> markiere Ziffernfolgen + optionales Interpunktionszeichen (eines aus: â€œ.;-:,â€) als NE ----------
    
    
    
    interpunkt = r"[.;\-:,]"
    digit_pattern = re.compile(rf"(\d+(?:{interpunkt})?)")

    processed_parts = []
    for item in final_parts:
        if item['ne'] or item['text'].isspace():
            processed_parts.append(item)
            continue
        s = item['text']
        # Wir splitten s in Sequenzen, wobei gefundene Ziffernfolgen als separate NE-Teile markiert werden
        last_idx = 0
        out_fragments = []
        #for m in mark_ext_digit_pattern.finditer(s):
        for m in digit_pattern.finditer(s):
            start, end = m.span(1)
            if start > last_idx:
                out_fragments.append({'text': s[last_idx:start], 'ne': False})
            out_fragments.append({'text': m.group(1), 'ne': True, 'type': 'number'})
            last_idx = end
        if last_idx < len(s):
            out_fragments.append({'text': s[last_idx:], 'ne': False})
        # falls nichts gefunden, out_fragments bleibt als original
        if not out_fragments:
            processed_parts.append(item)
        else:
            processed_parts.extend(out_fragments)

    #print("d: ", processed_parts)
    # ---------- Schritt 5: FÃ¼r alle als NE markierten Fragmente -> ersetze durch Marker â”œ<type>:<id>â”¤ ----------
    # Erstelle output-String schrittweise und fÃ¼lle mapping
    out = []
    for item in processed_parts:
        if item['ne']:
            orig = item['text']
            # hole id, die nicht bereits im Text vorkommt (banned_digit_sequences berÃ¼cksichtigt)
            nid = next_id()
            mapping[nid] = orig
            out.append(f"â”œ{nid}â”¤")
        else:
            out.append(item['text'])


    out_text = "".join(out)

    # ---------- Schritt 6: Ersetze "â”¤â”œ" durch "â”¤ â”œ" (also direkt aufeinanderfolgende NE-Tokens ggf. trennen) ----------
    out_text = out_text.replace("â”¤â”œ", "â”¤â”¿â”œ")
    out_text = out_text.replace("â”¤", "")
    out_text = out_text.replace("â”œ", "")

    # RÃ¼ckgabe: text mit Markern und mapping
    return out_text, mapping


def remove_markers(text_with_markers: str,
                      mapping: Dict[str, str]) -> str:
    """
    Ersetzt NE-Marker der Form â”œ<type>:<id>â”¤ im gegebenen Text durch die Originalsequenzen
    aus `mapping`.

    Args:
        text_with_markers: Text, der Marker wie â”œurl:3â”¤ enthÃ¤lt.
        mapping: Dict, das pro id die Originalsequenz liefert: mapping["3"] = "https://...",

    Returns:
        restored_text: Text, in dem alle Marker durch ihre Originalsequenzen ersetzt wurden
                       und Pseudo-Escapes zurÃ¼ckgetauscht sind.
    """

    # Pattern: â”œ<id>â”¤   -> wir extrahieren die id (alles bis zum nÃ¤chsten â”¤)


    #marker_re = re.compile(r"â”œ([^â”¤]+)â”¤")

    #found_ids = set()

    #def repl(m: re.Match) -> str:
    #    id_ = m.group(0)
    #    if id_ not in mapping:
    #            print(f"ID '{id_}' not found in mapping.")
    #            return m.group(0)
    #    found_ids.add(id_)
    #    val = mapping[id_]
    #    orig = str(val)
    #    return orig

    ## 1) Ersetze alle Marker durch die zugehÃ¶rigen Originalsequenzen
    #for id_ in mapping:
    #    marker_re = re.compile(id_)
    #    text_with_markers = marker_re.sub(repl, text_with_markers)

    ## 2) Falls bei der Ersetzung Pseudo-Escaped Marker zurÃ¼ckgegeben werden sollen, konvertiere sie:
    #restored = text_with_markers.replace(ESC_L, "â”œ").replace(ESC_R, "â”¤")
    #restored = restored.replace("â”¿", "")

    #if token_only:
        # match digit sequence that is a whole token (surrounded by whitespace or string boundaries)
    #    pattern = re.compile(r"(?<!\S)(\d+)(?!\S)")
    #else:
    pattern = re.compile(r"(\d+)")

    def _get_original_for_id(id_str: str) -> str:
        if id_str not in mapping:
            #if strict:
            print(f"ID '{id_str}' not found in mapping.")
                #raise KeyError(f"ID '{id_str}' not found in mapping.")
            return None  # caller will decide to leave unchanged
        val = mapping[id_str]
        if isinstance(val, dict):
            return val.get("text", "")
        return str(val)

    def repl(m: re.Match) -> str:
        id_str = m.group(1)
        orig = _get_original_for_id(id_str)
        if orig is None:
            # strict was False and id not found -> leave the digit sequence unchanged
            return id_str
        return orig

    restored = pattern.sub(repl, text_with_markers)

    restored = restored.replace(ESC_L, "â”œ").replace(ESC_R, "â”¤")
    restored = restored.replace("â”¿", "")
    #if restore_pseudo_escapes:
    #    restored = restored.replace(esc_left, "â”œ").replace(esc_right, "â”¤")




    return restored

if __name__ == "__main__":
    test_strings = [
		"hallo 1.1 2",
		"hallo  1.1  2",
		"1<unk>witaj</unk>ğŸ‘¨ğŸ§‘(ğŸ¥ª). Dies ist testğŸ¥ªa bernhard.baier@gmx.net. 1.",
		"bernhard1@gmx.net ;a543..4;:8-asasa123123",
		"bernhard1@gmx.net a543;5..3asasa123123",
		"dorostowa dÅºÄ›Å‚arniÄka a 25. lÄ›tne zeÅ„dÅºenje dÅºÄ›Å‚oweho kruha za nakrajne rumy NÄ›mskeje towarÅ¡nosÄ‡e za geografiju â€“ zhromadne zarjadowanje ze Serbskim institutom.",
		" dÅºÄ›Å‚arniÄka a 25. lÄ›tne geografiju â€“ zhromadne ;",
		"a543asasa123123",
		"<unk>witaj</unk>ğŸ‘¨ğŸ§‘(ğŸ¥ª). Dies ist testğŸ¥ªa",
		"<unk>witaj</unk>ğŸ‘¨ğŸ§‘(ğŸ¥ª). Dies ist testğŸ¥ªa bernhard.baier@gmx1.net. 47hallo11",
		"Bukowc je nÄ›hdÅºe Â 6km wulka a 88 metrow doÅ‚ha wjes w formje Å‚anowca (Waldhufendorf) a bu 1280 (mjeno naspomnjenja: Buchinwalde) prÄ›ni raz naspomnjeny. Mjeno wjeski pokazuje na sydliÅ¡Ä‡o pÅ™i bukowym lÄ›su. Nimo ryÄ‡erkubÅ‚a bÄ›chu 1777 teÅ¾ hiÅ¡Ä‡e 6 burskich statokow, 20 chÄ›Å¾karjow a 14 zahrodkowych Å¾iwnosÄ‡erjow, pola a Å‚uki w Bukowcu. SrjedÅº 19.lÄ›tstotka leÅ¾achu wokoÅ‚o Bukowca 11 hatow z cyÅ‚kownej pÅ‚oninu 40 hektarow a w kotrychÅ¾ plahowachu so karpy.",
		"PÅ™ejemy wam teÅ¾ hiÅ¡Ä‡e wÅ¡o dobre za #20230#, krutu strowotuğŸ, wjele lubosÄ‡eğŸ’ a Äasa za so a teÅ¾ wjele wjeselağŸ˜Š a rjanych doÅ¾iwjenjow ze swÃ³jbu a pÅ™eÄ‡elemiğŸ«‚.",
		"Wir wÃ¼nschen euch auch noch alles Gute fÃ¼r #20230#, beste GesundheitğŸ, viel Liebe #22# und Zeit fÃ¼r uns und auch viel SpaÃŸ ğŸ˜Š und schÃ¶ne Erlebnisse mit Familie und FreundenğŸ«‚"
    ]

    for test_string in test_strings:
        print(test_string)
        print("-------------\nescaped:")
        escaped, mapping = set_markers(test_string)
        print(escaped)
        print("-------------\nbacktranslated:")
        print(remove_markers(escaped, mapping))
        print("\n\n\n")
